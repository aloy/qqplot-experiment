\documentclass{article} % JASA requires 12 pt font for manuscripts
%\usepackage{JASA_manu}        % For JASA manuscript formatting
\usepackage{times, asa}

%\usepackage{endfloat} % just for while I am writing

% for citations
\usepackage[authoryear]{natbib} % natbib required for JASA
\usepackage[colorlinks=true, citecolor=blue, linkcolor=blue]{hyperref}

% for the fancy tables with the icons
%\usepackage[margin=1.0in]{geometry}% http://ctan.org/pkg/margin
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{array}% http://ctan.org/pkg/array
\newcolumntype{M}{>{\centering\arraybackslash}m{\dimexpr.05\linewidth-2\tabcolsep}}



%\definecolor{Blue}{rgb}{0,0,0.5}
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\al}[1]{{\color{red} #1}}

\newcolumntype{C}[1]{>{\centering}m{#1}}
% fonts
%\usepackage{kpfonts}

% for figures
\usepackage{graphicx}
\DeclareGraphicsExtensions{.eps, .pdf}
\graphicspath{{figures/}}

\usepackage{wrapfig,float}
\usepackage{caption}
\usepackage{subcaption}

% help with editing and coauthoring
\usepackage{todonotes}
\newcommand{\alnote}[1]{\todo[inline,color=green!40]{#1}}
\newcommand{\hhnote}[1]{\todo[inline,color=magenta!40]{#1}}

% title formatting
% \usepackage[compact,small]{titlesec}

% page formatting
\usepackage[margin = 1in]{geometry}
% \usepackage[parfill]{parskip}

% line spacing
\usepackage{setspace}
\doublespace

% For math typsetting
\usepackage{bm}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}

% A few commands to make typing less tedious
\newcommand{\inv}{\ensuremath{^{-1}}}
\newcommand{\ginv}{\ensuremath{^{-}}}
\newcommand{\trans}{\ensuremath{^\prime}}
\newcommand{\E}{\ensuremath{\mathrm{E}}}
\newcommand{\var}{\ensuremath{\mathrm{Var}}}
\newcommand{\cov}{\ensuremath{\mathrm{Cov}}}


\title{Variations of Q-Q Plots -- the eyes have it!}

\author{Adam Loy, Lendie Follett, Heike Hofmann
\thanks{Adam Loy is an Assistant Professor in the Department of Mathematics, Lawrence University, Appleton, WI, 54911 (e-mail: adam.m.loy@lawrence.edu);  Lendie Follett is a Ph.D. student in the Department of Statistics and Statistical Laboratory, Iowa State University, Ames, IA 50011-1210.; Heike Hofmann is a Professor in the Department of Statistics and Statistical Laboratory, Iowa State University, Ames, IA 50011-1210. This work was funded in part by National Science Foundation grant DMS 1007697. All data  in the study was collected  with approval from the internal review board IRB 10-347.}}

\begin{document}

\maketitle
\begin{abstract}
this paper holds two messages: 
a) our eyes are well suited to assess distributional assumptions. We  objectively measure  power and sensitivity of Q-Q plots using lineup tests. 
At the example of normal distributions we can show that Q-Q plots are better at identifying non-normality than some prominent tests of normality, such as Shapiro-Wilks,  Anderson-Darling,  Kolmogorov-Smirnov, Lilliefors, or  Cramer-von-Mises.

b) Out of the variations discussed, de-trended Q-Q plots perform significantly worse than the other two variants. This is surprising, because cognitive theory tells us, that de-trended plots should be better suited in assessing the difference between empirical and hypothesized  distribution.
\end{abstract}
\keywords{Quantile-Quantile plot, Normality test, Statistical graphics, Lineup protocol, Visual inference}

<<setup, echo=FALSE, results='hide'>>=
suppressWarnings(library(ggplot2))
library(knitr)
opts_chunk$set(cache=TRUE, fig.width=4, fig.height=4)
@

<<data, echo=FALSE, results='hide', message=FALSE>>=
turk <- read.csv("data/turk-10-13.csv")
suppressMessages(require(plyr))
params <- ldply(strsplit(split="-", as.character(turk$test_param)), function(x) x)
turk$treatment <- params$V2
turk$placement <- params$V3
turk$choice <- params$V4
turk$replicate <- params$V5
turk$df <- as.numeric(llply(strsplit(split="-", as.character(turk$param_value)), function(x) x[2]))

library(lubridate)
turk$start_time <- ymd_hms(turk$start_time)
turk <- ddply(turk, .(ip_address), transform, 
              attempt=rank(start_time, ties.method="random"),
              numeval=length(start_time))

turk$weight <- ldply(strsplit(as.character(turk$response_no), split=","), length)$V1
# kick out responses without an answer:
turk <- subset(turk, weight!=0)
# take out participants with fewer than 10 responses:
turk <- subset(turk, numeval >= 10)


res <- llply(turk, function(x) rep(x, turk$weight))
turkw <- as.data.frame(res)
turkw$weight <- 1/turkw$weight
turkw$response <- unlist(strsplit(as.character(turk$response_no), split=","))

turkw$correct <- turkw$response == turkw$plot_location
turkw <- subset(turkw, df != 0) # exclude Adam's data


turkw$sample_size <- factor(turkw$sample_size)
turkw$df <- factor(turkw$df)
@
<<data15, echo=FALSE, results='hide'>>=
t15<- read.csv("data/turk15.csv") #turk-10-13
params <- ldply(strsplit(split="-", as.character(t15$test_param)), function(x) x)
t15$treatment <- params$V2
t15$placement <- params$V3
t15$choice <- params$V4
t15$replicate <- params$V5
t15$df <- as.numeric(llply(strsplit(split="-", as.character(t15$param_value)), function(x) x[2]))

library(lubridate)
t15$start_time <- ymd_hms(t15$start_time)
t15 <- ddply(t15, .(ip_address), transform, 
              attempt=rank(start_time, ties.method="random"),
              num_attempt=length(start_time))

t15$weight <- ldply(strsplit(as.character(t15$response_no), split=","), length)$V1
# kick out responses without an answer:
t15 <- subset(t15, weight!=0)

res <- llply(t15, function(x) rep(x, t15$weight))
t15w <- as.data.frame(res)
t15w$weight <- 1/t15w$weight
t15w$response <- unlist(strsplit(as.character(t15$response_no), split=","))

t15w$correct <- t15w$response == t15w$obs_plot_location

t15w$sample_size <- factor(t15w$sample_size)
t15w$df <- factor(t15w$df)
@

<<functions, echo=FALSE>>=
sim_env <- function(x, conf = .95, line=FALSE){
  n <- length(x)
  P <- ppoints(x)[rank(x)]
  z <- qnorm(P)
  if (line) {
    require(HLMdiag)
    a <- as.numeric(HLMdiag:::qqlineInfo(x)[1])
    b <- as.numeric(HLMdiag:::qqlineInfo(x)[2])
  } else {
    a <- 0  # we know that the line should be the identity
    b <- 1
  }
  zz <- qnorm(1 - (1 - conf)/2)
  SE <- (b/dnorm(z)) * sqrt(P * (1 - P)/n)
  fit.value <- (a + b * z)
  upper <- (fit.value + zz * SE)
  lower <- (fit.value - zz * SE)
  return(data.frame(lower, upper, fit.value, idx=1:length(x)))
  
}

std_lineup <- function(dframe) {
  require(ggplot2)
p <- ggplot(aes(x=naive1.qq.x, y=naive1.qq.y), data = dframe) + 
          geom_smooth(aes(naive1.qq.x, naive1.env.fit.value), colour="grey50", se=FALSE, method="loess") +
        #  geom_abline(colour="grey50") +
          facet_wrap(~.sample, ncol=5) +
          geom_point() + 
          geom_ribbon(aes(x = naive1.qq.x, ymin = naive1.env.lower, ymax = naive1.env.upper),alpha = .2)+
          labs(x = "", y = "") +
          theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
                axis.ticks = element_blank())
  p  
}


rot_lineup <- function(dframe) {
  require(ggplot2)
  print(  ggplot(aes(x=naive1.qq.x, y=naive1.qq.y-naive1.env.fit.value), data = dframe) + 
            facet_wrap(~.sample, ncol=5) + 
            geom_hline(yintercept=0, colour="grey30")+
            geom_point() + 
            geom_ribbon(aes(x = naive1.qq.x, 
                            ymin = naive1.env.lower-naive1.env.fit.value, ymax = naive1.env.upper-naive1.env.fit.value),alpha = .2)+
            labs(x = "", y = "") +
            theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
                  axis.ticks = element_blank()))
}

ctrl_lineup <- function(dframe) {
  require(ggplot2)
  print(ggplot(aes(x=naive1.qq.x, y=naive1.qq.y), data = dframe) + 
          geom_smooth(aes(naive1.qq.x, naive1.env.fit.value), colour="grey50", se=FALSE, method="loess") +
          geom_point() + 
          facet_wrap(~.sample, ncol=5) +
          labs(x = "", y = "") +
          theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
                axis.ticks = element_blank())
  )
}
@


%------------------------------------------------------------------------------------
\section{Introduction}
%------------------------------------------------------------------------------------
\hhnote{Why do we need to assess distributional assumptions in the first place?
Verify that distributional assumptions hold, check model results (such as residuals), ... XXX ...
Impact of violation? wrong or overly confident predictions, XXX
}

Standard quantile-quantile (Q-Q) plots \citep{Wilk:1968} are an essential tool for  visually evaluating a specific distributional assumption. In a Q-Q plot we plot quantiles of the empirical distribution against the expected quantiles from the assumed distribution. 
The line of identity therefore represents the theoretical distribution and points show the empirical distribution. 
\alnote{I am having trouble with the last sentence. The line of identity represents agreement between the two distributions and the points represent the comparison. I know what we're trying to say here, and I think the issue is that the current wording conditions on the data point.}
\hhnote{OK, I can see what you mean - but I still believe that we can think of the line as representing the graph of the theoretical distribution. What is important is to note, that the line is determined completely by the theoretical distribution and does not rely on the sample at all.  This is going to be a crucial point later on in the rescaling discussion. I'll think about how to clarify this more. Here, I just wanted to introduce Q-Q plots quickly and then go into a more formal explanation later on.}

Deviations from the theoretical distribution then manifest themselves as vertical differences between points and the line of identity. This difference is featured in a series of distributional tests. More formally, let $F_n$ be the empirical distribution function (ECDF) based on a sample size of $n$, and $F$ be the hypothesized/true distribution. The absolute difference between the two distribution functions for each sample point, $\left| F_n(x_i) - F(x_i) \right|$, is then the main contributor for the test statistics of the Kolmogorov-Smirnov \cite[KS-test,][]{kolmogorov:1933, smirnov:1948}, the Lilliefors \cite[LF-test, ][]{lilliefors}, the Anderson-Darling \citep[AD-test,][]{adtest:1954}, and the Cram\'{e}r-von-Mises test \citep[CVM-test,][]{cramer:1928, mises:1928}, as shown in table~\ref{tab:tests}.

The KS test uses the maximal  difference, which is displayed as the maximal vertical extent between the line of identity and the data points in a Q-Q plot, regardless of the range of the sample, i.e. a difference $D$ observed at either tail of the distribution carries the same weight and is interpreted in the same way as a difference $D$ in the center of the distribution. While the KS test allows for adjusting parameters of the normal distribution to sample mean and variance, it is more appropriate to use LF for this purpose. LF and KS share the same test statistic, but the \al{sampling distribution} in the LF test \al{statistic} is adjusted for the two additional parameters.  AD and CVM  are both based on the total area between the line of identity and the empirical distribution function. Compared to the KS  test,  the CVM test downplays the effects in the tails of a (normal) distribution, while Anderson-Darling upregulates the tail effect using a weighting of $1/\left(F(x)(1 - F(x)\right)$ across the range of the sample. 


\begin{table}
\centering
\begin{tabular}{lrl}\hline
Test && Statistic\\\hline\hline
Kolmogorov-Smirnov & $D =$ & $ \sup_{1 \le i \le n} \left | F_n(x_i) - F(x_i)\right|$ \\
Lilliefors & $D =$ & $ \sup_{1 \le i \le n} \left | F_n(x_i) - F(x_i)\right|$ \\
Anderson-Darling & $A =$ & $ n \int_{-\infty}^{+\infty} \left | F_n(x) - F(x)\right|^2/\left(F(x)(1 - F(x)\right) dF(x)$\\
Cram\'{e}r-von-Mises & $C =$ & $n \int_{-\infty}^{+\infty} \left | F_n(x) - F(x)\right|^2 dF(x)$ \\\hline
\end{tabular}
\caption{\label{tab:tests} Four prominent tests for normality based on the difference between empirical and hypothesized distribution function. An overview of the performance and power of these tests can be found in \citet{stephens:1974}.}
\end{table}
%

% The Shapiro-Wilk test \cite[SW-test][]{Shapiro:1965kt} does not fit this scheme, but has been shown to be the most powerful in assessing non-normality \citep{stephens:1974, razali:2011}. 
\al{The Shapiro-Wilk test \cite[SW-test][]{Shapiro:1965kt} does not utilize deviations from the theoretical distribution function, rather it focuses on the linearity of a normal Q-Q plot. Under normality, a set of observations, $x_1, \ldots, x_n$, can expressed as $x_i = \mu + \sigma z_i$, where $z_i$ is a quantile from the standard normal distribution. The Shapiro-Wilk test compares (up to a constant of proportionality, $c$) two estimates for $\sigma$: the best linear unbiased estimate obtained from a generalized least squares regression of the sample order statistics on their expected values, denoted $\widehat{\sigma}$, and the sample standard deviation, $s$.
\[
  W = \frac{(c \widehat{\sigma})^2}{s^2} = \frac{b^2}{s^2}
\]
For a sample drawn from a normal distribution, $b^2$ and $s^2$ are, up to a constant, estimating the same quantity, whereas the two estimators will generally not be estimating the same quantity under nonnormality. The SW test has been shown to be the most powerful in assessing non-normality \citep{stephens:1974, razali:2011}. }


We will be making use of these tests to assess the effectiveness of different variations of standard Q-Q plots.

\alnote{As I was rereading I felt that the below paragraph seemed out of place. I don't know where else it would go yet...}
 A Q-Q plot of sample $x$ is constructed by plotting theoretical quantiles $F^{-1}(F_n(x_i))$ against sample quantiles $x_{(i)}$. In the case that the empirical distribution $F_n$ is consistent with distribution $F$, the points in the Q-Q plot falls on the line of identity. 
For any sample tested against a distribution within a location-scale family, such as e.g.~normal, log normal, or exponential, the sample quantiles still fall on a line when plotted against the theoretical quantiles of any of the family's member distributions. Plotting the empiricial quantiles of a normally distributed sample $x \sim N(\mu, \sigma^2)$ against the quantiles of a standard normal will result in a line, where  the slope is an estimate of $\sigma$, and the intercept estimates $\mu$. Visually  the only change in the Q-Q plot is a  change in the scale of the $y$-axis. We can therefore employ Q-Q plot in the more general framework of testing the distribution of a sample $x$ for normality similar to standard normality tests, such as the AD, LF, CVM, and SW test. We do have to make a decision with respect to the exact parameters of the normal distribution we test against, when we plot a line alongside the points in the Q-Q plot for additional comparison purposes, i.e.~parameters $\mu$ and $\sigma$ have to be estimated from the sample. In Q-Q plots, variability is based on a robust measure of spread given as the ratio of the inter-quartile ranges (IQRs) of empirical and theoretical distributions: $\left(F^{-1}_n(0.75) - F^{-1}_n(0.25)\right) / \left(F^{-1}(0.75) - F^{-1}(0.25)\right)$ \citep[see e.g.~][\hh{p.~XXX}]{tukey:eda}. 
 
%\hhnote{How are Q-Q plots interpreted?}

%\hhnote{slope is standard deviation of theoretical distribution, $y$ offset its location. For any distribution within a location-scale family (such as normal, log normal, exponential, ...), we can interpret shifts in $y$ direction as a change/deviation in location, and a change in slope as a change in scale.}

A sample $x$ is considered as being consistent with a normal distribution, if empirical and theoretical quantiles fall close to  a straight line.  Closeness can  be assessed visually based on whether the points fall inside the envelope of 95\%  pointwise confidence intervals \citep[][\al{p.~XXX}]{Davison:1997}.
\alnote{We need to be careful here. Since we are using pointwise CIs, we would expect a certain \% of points to fall outside the CIs. I think in the first review of the JCGS paper a reviewer was concerned about this as well.}
\hhnote{Well, yes, point taken,  but it is also important  how far outside the points are ... }
\alnote{I think the Davison and Hinkley reference takes care of this. I will reread that section and add a page number soon.}

In assessing differences between points and lines, onlookers have a tendency to evaluate shortest, i.e.~orthogonal, distance, even when asked to evaluate differences based on vertical distance \citep{sineillusion, robbins:2005, cleveland:1984}. 

\alnote{Do we want a reference for the de-trended Q-Q plots? The only reference I can think of was in the Thode book on testing normality. I think it is in the Snedecor reading room, or the ISU library.}
In so-called {\it de-trended Q-Q plots} the $y$ axis is changed to show the difference between theoretical and sample quantiles. The line of the theoretical distribution therefore coincides with the $x$ axis. Vertical differences between empirical and theoretical distribution therefore  coincide with orthogonal distance. 

De-trending should therefore aid the visual assessment between empirical CDF and theoretical CDF. This also follows the general standard graphical recommendation to directly plot the aspect of the data we want to show rather than asking audiences to derive it \citep{wainer:2000}.  Another point in favor of this design is that it makes better use of the available space.



In this paper we want to investigate the effectiveness and the power of the modifications made to Q-Q plots.


Examples of the three versions of Q-Q plots under considerations are displayed in Figure~\ref{qqplots} and include (from left to right): a \emph{control} Q-Q plot, a \emph{standard} Q-Q plot with an added grey band representing a 95\% pointwise confidence region \citep{Davison:1997}
%\alnote{I think pointwise bands are described in Atkinson (1985). Meeker uses simultaneous confidence bands for P-P plots. The motivation to use pointwise in the lineups seems to be that it is what people do (i.e., JMP (I think). qqPlot in the car package in R, etc.)}
based on the estimated standard error of the order statistics for an independent sample from the theoretical distribution, and a \emph{de-trended} Q-Q plot. All Q-Q plots in Figure~\ref{qqplots} are constructed from the same data. 

\begin{figure}
\centering
% <<qqplots, dependson='functions', fig.width=2.75, fig.height=2.75, out.width='0.3\\textwidth', echo=FALSE, include=TRUE>>=
% dframe <- read.csv("lineup-data/data-1-1-1-20-2-14-5.csv")
% library(ggplot2)
% dframe$.sample <- "Control"
% ctrl_lineup(subset(dframe, .sample_outer==5))
% dframe$.sample <- "Standard"
% std_lineup(subset(dframe, .sample_outer==5))
% dframe$.sample <- "De-trended"
% rot_lineup(subset(dframe, .sample_outer==5))
% @
\includegraphics[width=0.3\textwidth]{qqplots1}
\includegraphics[width=0.3\textwidth]{qqplots2}
\includegraphics[width=0.3\textwidth]{qqplots3}
\caption{ \label{qqplots} Three versions of Q-Q plots: control, standard, and de-trended.}
\end{figure}



In order to objectively evaluate  the three designs and quantify their effectiveness we make use of {\it lineup tests}.

Lineup tests have been introduced by \citet{buja:2009hp} to evaluate and quantify the significance of graphical findings. The idea behind a lineup test is that of a police lineup: the chart of the observed data is placed randomly among a set of so called null charts, showing data created consistently with the null hypothesis. In the setting of a lineup of normal Q-Q plots, the null hypothesis  is either that F is standard normal or F is normal with parameters based on sample mean and variance.
If the `suspect' -- i.e. the plot of the observed data -- can be identified from the null charts, this counts as evidence against the null hypothesis. Multiple identifications of the data by independent observers then lead to a rejection of the null hypothesis. 
The lineup protocol also allows for an assessment of the power of a lineup \citep{mahbub:2013},  
%as the probability that in $N$ independent evaluations observers 
and by showing different renderings of the exact same data in lineups we can evaluate the power  of different designs.
In considering the power of a lineup, we need to estimate the probability $p_i$ that observer $i$ identifies the data from the lineup. If the observer is just guessing, this probability is $1/m$, where $m$ is the number of plots in the lineup.
The power of a lineups is then given as the probability to reject the null hypothesis. Let $Y$ be the number of identifications of the data plot in $N$ independent evaluations. Let $Y \sim F_N$. The power of the lineup is then given as the probability that more than $y_\alpha$ out of $N$ observers
choose the true plot, i.e.:
\begin{equation}\label{eqn:power}
\widehat{\text{Power}} = \text{Power}_{N} = 1 - F_{Y} (y_{\alpha}),
\end{equation}
where $y_\alpha$ is the critical value for a given significance level of $\alpha$, i.e.~$P(Y >  y_{\alpha}) \le \alpha$. $Y$ is composed of the sum of $N$ observers' (binary) decisions $Y_i \sim B_{1, p_i}$, where  $p_i$ is the probability that individual $i$ chooses the data plot. This probability  depends both on the strength of the signal in the data plot and an individual's visual ability.
Assessing this ability requires that each individual evaluates multiple lineups. 
If that is not possible, we have to assume that all participants share the same ability $p$. %, and the power calculation in Equation~\ref{eqn:power} simplifies to $1 - B_{N, \hat{p}}(x_\alpha)$, where $\widehat{p}$ is an estimate for the probability of choosing the data plot for a specific lineup.
Similar to classical inference, we can make use of power to assess sensitivity of tests. This allows us to make decisions about designs for particular tasks by evaluating lineups displaying  the same data in different types of displays \citep{Hofmann:2012ts}. 

\hhnote{Outline of the paper:}
\hh{
\begin{itemize}

\item We model power of different lineups in a simulation setting: based on 24 samples from $t$ distributions with different degrees of freedom and varying sample size (section~\label{sec:simu}), we draw lineups for all three designs of Q-Q plots and evaluate their respective power in section~\ref{sec:power1}. 
\item A further power comparison follows in section~\ref{sec:power2}: using   visual $p$-values to assess sensitivity of lineups of Q-Q plots we can make direct comparisons to other tests of normality, such as KS, LF, ...
\end{itemize}}


%------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Simulation Setup and Model}\label{sec:simu}
%------------------------------------------------------------------------------------

To further develop the assessment of normality using lineups, we conducted a study comparing the three different versions of the Q-Q plot.
%We are testing three different versions of a Q-Q plot, 


To investigate the power of the three different Q-Q plot versions, we sample data from a $t$ distribution with varying degrees of freedom and sample sizes, and include a Q-Q plot of this data in a lineup of null charts drawn from standard normal samples of the same size.
For lineup tests it is of extreme importance to consider the generation of the null sets and the construction of the plots in the lineup. 
Null data is created conistently with the null hypothesis. Here, we have two different null hypotheses to consider:

\begin{tabular}{lp{4.75in}}
\multicolumn{2}{l}{\bf Situation I:}\\
$H_0: F_n = N(0,1)$ &  null samples are drawn from a standard normal distribution \\
& reference line is the identity. Lines and envelopes are the same across all panels, in particular, all panels have the same scale. \\[3pt]
\multicolumn{2}{l}{\bf Situation II:}\\
$H_0: F = N(0,S^2)$ & $S$ is based on the interquartile range of the data; \\
& null samples are drawn from $N(0, S^2)$. \\
& The reference line has a slope of $S$ (and an intercept of 0).  All panels have the same scale. 
\end{tabular}

\alnote{The generation of the null data makes sense to me, but is there a way to use bullet points on the right side of the table? That would help me read it...}

Examples for both hypotheses are shown in figure~\ref{fig:lps}. Both lineups show the same dataset (in panel \#$(3^2-3)$). On the left the data stands out (all 33 observers picked the data plot), i.e.~we reject the null hypothesis of a standard normal distribution. On the right, the data does not stand out (only 3 out of 27 observers picked the data) -- we do not reject the hypothesis of a normal distribution with parameters $\mu=0$ and $\widehat{\sigma}=1.578$. 
\alnote{Based on our last JCGS submission I could see a reviewer asking why this is. Of course, I believe that a seasoned data analyst would agree with us, so I am unsure how much detail we need.}

Note that the above list of hypotheses is not exhaustive. Any theoretical distribution in Q-Q plots corresponds to a  hypothesis test against that distribution. \hh{As long as there is a method to generate samples under the null hypothesis, we do not even need to know the exact distribution. This allows us in particular to assess situations in which we only have approximate or asymptotic results, which are hard if not impossible to investigate with the (small) finite samples we typically deal with in practice.}
Note that IQR is used here in estimating scale - this is standard for Q-Q plot. A robust estimation of variance is preferred for better assessment of tails and outliers of the empirical distribution. We could use alternative estimators, such as median absolute deviation (MAD) or adjusted MAD \citep{rousseeuw}, for variance, but this will likely also change the power of the corresponding lineup.

\begin{figure}[hbt]
<<lps-dsn, dependson='functions', echo=FALSE, message=FALSE>>=
dframe <- read.csv("lineup-data/data-1-2-15-50-2-13-6.csv")
dframe$.sample <- dframe$.sample_outer

fit2 <- dframe
b <- (HLMdiag:::qqlineInfo(dframe$x[dframe$.n==20]))[2] # get robust estimate for sigma
#sd(fit2$x[fit2$.n==20])
fit2$x[fit2$.n==20] <- dframe$x[dframe$.n==20]/b # change variance to 1, compare then
idx <- grep("naive1.qq", names(fit2))
fit2 <- ddply(fit2[,-idx], .(.n), transform, naive1.qq=qqnorm(x, plot.it=FALSE))
idx <- grep("naive1.env", names(fit2))
fit2 <- ddply(fit2[,-idx], .(.n), transform, naive1.env=sim_env(x))
@
\begin{subfigure}{0.5\textwidth}
<<echo=FALSE, fig.width=7, fig.height=7>>=
std_lineup(dframe)
@
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
<<echo=FALSE, fig.width=7, fig.height=7>>=
std_lineup(fit2) 
@
\end{subfigure}
\caption{\label{fig:lps} Lineup plots of standard Q-Q plots. The observed data is the same, but  reference lines and envelopes are based on a standard normal distribution on the left; while  reference lines and envelopes for the lineup on the right are based on a normal distribution $N(0, \widehat{S}^2)$, where $\widehat{S}$ is based on the IQR of the observed data.
The observed data in both lineups is displayed in panel \#$(3^2 - 3)$. }
\end{figure}

Next, we model the aforementioned probability $p_i$ with which observer $i$ picks the true data from a lineup. 
%To explore the results of this study we must first define some additional notation.

%\alnote{We can give a description of the MTurk study here.}

%------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------


Let $X_i \sim B_{1, \pi_i}, 1 \le i \le n$, where $X_i$ is the binary decision on the $i$th evaluation and $\pi_i$ is the probability with which the observer chooses the data plot. This probability is influenced by a number of factors:

\begin{center}
\begin{tabular}{lp{5in}}
$\tau$ & the design used in the lineup (Control, Standard, De-trended), \\
&  the specific parameters under which the data for the lineup were created: \\
&  $\delta$ \ \ \ degrees of freedom (2, 5, 10) {of the $t$ distribution} and \\
&  $\nu$  \ \ \ sample size (20, 30, 50, 75), \\
$d$ &  the level of difficulty based on the actual sample, and \\
$u$ & the users' subjective abilities.
 \end{tabular}
\end{center}
%

%
The combination of different levels of sample size and degrees of freedom of the $t$ distribution result in 12 parameter settings. Under each setting, we  generated data twice; additionally, we make use of two different sets of null data for each sample, yielding 48 different sets of data, which we render in each of the variations, resulting in 144 different lineups. 


Using  Amazon MTurk \citep{amazon}, \Sexpr{length(unique(turk$ip_address))} independent observers were recruited and asked to evaluate ten lineups each. 
%
Half of the lineups that observers were shown allowed multiple choices of plots from a lineup for the final answer. While most participants still chose only a single plot, in the analysis we dealt with multiple answers to a lineup by using a weighting variable with reciprocal values of the number of answers given by a participant.

<<rejects, dependson='data', echo=FALSE, message=FALSE>>=
lps <- ddply(turkw, .(data_name, treatment), summarise,
             correct=sum(correct*weight),
             num=sum(weight)
             )

parse_filename <- function(fname) {
  pars <- strsplit(fname, "-")[[1]]
  names(pars) <- c("data", "null", "rep",  "i", "n", "df", "innerPos", "outerPos")
  pars
}

exps <- ldply(as.character(lps$data_name), parse_filename)
lps<-data.frame(lps, exps)
require(vinference)
lps$pvals <- unlist(llply(1:nrow(lps), function(i) {
  correct <- floor(lps$correct[i])
  n <- lps$num[i]
  vinference:::hdistribution(correct, n, m=20)
}))
lps$signif <- lps$pvals < 0.05

@

<<rejects15, dependson='rejects', echo=FALSE, message=FALSE>>=
lps15 <- ddply(subset(t15w, num_attempt>=10), .(data_name, treatment), summarise,
             correct=sum(correct*weight),
             num=sum(weight),
             avg=sum(weight)/length(weight)
             )

lps15$pvals.adj <- unlist(llply(1:nrow(lps15), function(i) {
  correct <- floor(lps15$correct[i])
  n <- lps15$num[i]
  vinference:::hdistribution(correct, n, m=20)
}))

lps <- merge(lps, lps15[, c("data_name", "treatment", "pvals.adj")], by=c("data_name", "treatment"), all=TRUE)
@


<<triples, echo=FALSE, warning=FALSE, message=FALSE, results='hide'>>=
library(reshape2)
lps$prop <- with(lps, correct/num)
dt <- dcast(lps, data_name~treatment, value.var="prop")
exps <- ldply(as.character(dt$data_name), parse_filename)
exps$data_name <- as.character(dt$data_name)
dt <- merge(dt, exps, by="data_name")
dt$df <- as.factor(dt$df)
dt$df <- factor(dt$df, levels=c(2,5,10))


pvals <- dcast(lps, data_name~treatment, value.var="signif")
names(pvals) <- gsub("^", "sg.",names(pvals))
names(pvals)[1] <- "data_name"

dt <- merge(dt, pvals, by="data_name")
dt$sg.Standard <- factor(dt$sg.Standard)
dt$sg.Standard <- factor(dt$sg.Standard, levels=c("TRUE", "FALSE"))
dt$df <- factor(dt$df, levels=c("10", "5", "2"))
@
<<nortests, echo=FALSE>>=
pvals <- read.csv("data/pvalues.csv")
lps$pvals.adj[is.na(lps$pvals.adj)] <- lps$pvals[is.na(lps$pvals.adj)]
lppvals <- dcast(lps, data_name~treatment, value.var="pvals.adj")

lppvals <- merge(lppvals, pvals, by="data_name")
lppvals <- merge(lppvals, exps, by="data_name")

#cor(lppvals[,c("Standard", "Control", "Rotated", "cvm", "ad", "ks", "sw", "lf")])
#ggpairs(lppvals[,c("Standard", "Control", "Rotated", "cvm", "ad", "ks", "sw")])
lppvals$df <- factor(lppvals$df)
lppvals$df <- factor(lppvals$df, levels=c("10", "5", "2"))

require(reshape2)
lppvalscb <- dcast(lppvals, ad+sw+lf+cvm+ks+rep+n+df~null, value.var="Standard")
lppvalscb$Standard <- with(lppvalscb, (`1`+`2`)/2)

p <- ggplot() + 
  geom_abline(colour="grey70") + 
  geom_rect(aes(xmin=-0.02, xmax=1, ymin=-0.02, ymax=0.05), fill="grey70", alpha=0.5) + 
  geom_rect(aes(xmin=-0.02, xmax=0.05, ymin=-0.02, ymax=1), fill="grey70", alpha=0.5) + 
  geom_point(aes(x=Standard, y=sw, colour=df, shape=df), size=3, data=lppvalscb) + 
  scale_colour_brewer("degrees of\nfreedom", palette="Set2") + theme_bw() +scale_shape("degrees of\nfreedom") + ylim(c(-0.02,1)) + geom_point(aes(x=Standard, y=sw), pch=21, size=7, data=subset(lppvalscb, (Standard < 0.03 & sw > 0.2) |(Standard > 0.05 & sw < 0.05))) + theme() + ylab("Shapiro-Wilks (p-values)") + xlab("Standard Q-Q plot lineup (p-values)") 
                                                                                                                                                                                                                                                                                                        
#suppressWarnings(p)
ggsave(plot=p, filename="figures/figvisnorm.pdf", width=5.5, height=4)
# data-1-2-17-20-5-8-2.csv  # Standard significant
# data-2-2-17-20-5-8-15.csv
# data-1-2-23-50-10-8-10.csv  # Standard not significant
# data-2-1-6-30-5-14-15.csv
@
\begin{figure}
\centering
\begin{subfigure}[b]{.3\textwidth}
<<dependson='triples', echo=FALSE>>=
ggplot() + geom_abline(colour="grey50") +  geom_point(aes(x=Rotated, y=Standard, colour=df, shape=interaction(df,sg.Standard), size=sg.Standard), data=dt) + theme_bw() +scale_colour_brewer(palette="Set2") + theme(legend.position="none") + scale_shape_manual(values=c( 15,16,17, 22,21)) + scale_size_manual(values=c(3,4))
@
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
<<dependson='triples', echo=FALSE>>=
ggplot() + geom_abline(colour="grey50") +  geom_point(aes(x=Rotated, y=Control, colour=df, shape=interaction(df,sg.Standard), size=sg.Standard), data=dt) + theme_bw() +scale_colour_brewer(palette="Set2") + theme(legend.position="none") + scale_shape_manual(values=c( 15,16,17, 22,21)) + scale_size_manual(values=c(3,4))
@
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
<<dependson='triples', echo=FALSE>>=
ggplot() + geom_abline(colour="grey50") + geom_point(aes(x=Control, y=Standard, colour=df, shape=interaction(df,sg.Standard), size=sg.Standard), data=dt) + theme_bw()  + scale_size_manual(values=c(3,4), guide="none") +scale_colour_brewer("degrees of\nfreedom",palette="Set2", guide="none") + theme(legend.position=c(0.75, 0.20),legend.background = element_rect(fill = "white", colour = NA)) + scale_shape_manual("degrees of freedom\n(significance)", values=c(  15,16,17, 22,21), labels=c("10 ","5 ","2 ","10 (n.s.)", "5 (n.s.)")) + guides(shape=guide_legend(ncol=2, override.aes = list(size = c(3,3,3,4,4), colour=c("#66C2A5", "#FC8D62", "#8DA0CB", "#66C2A5", "#FC8D62"))))
@
\end{subfigure}
\caption{\label{fig:compare}Proportions of successful evaluation of the same data in the three different variations of Q-Q plots. Standard and control displays exhibit the highest correlation. De-trended Q-Q plots agree with decisions made based on Q-Q plots in the control or standard design, but display lower rates of correct responses in the `middle' field. Significances are based on lineup evaluations in the standard design. }
\end{figure}



Figure~\ref{fig:compare} shows proportions of correct evaluations of the lineups under the three different variations of Q-Q plots. All three versions provide highly correlated results, and largely coincide for extreme decisions (all correct/all wrong evaluations). In the middle range de-trended Q-Q plots perform  worse than either standard or control Q-Q plots. Lineups including data samples from a $t_2$-distribution  are all rejected in lineups under the standard design, while most of the $t_{10}$ samples go undetected. Lineups showing a sample from a $t_5$ distribution cover the whole range.
\hh{We base an evaluation of the three different Q-Q plot designs on the premise that if participants find it easier to identify the data plot under one lineup over another lineup (given identical data underlying the lineups), the first lineup uses the better design.}

\hh{Let therefore $Y_i$ be the outcome of the $i$th evaluation. Then $Y_i$ is a Bernoulli variable, where $\pi_i$ denotes the probablity of identifying the data plot from the lineup; i.e.~$P(Y_i = 1) = \pi_i = E[Y_i]$.   }

\hh{The probability of identifying the lineup is affected by several factors: (a) the strength of the signal, i.e.~degrees of freedom of the $t$ distribution, and the sample size, (b) a human factor, i.e~the visual ability of the observer, and (c)  the `lineup factor': depending on which $m-1$ representatives of the test distribution the null plots show, lineups of the same data plot can have different difficulty. We capture all of this in a logistic regression with fixed effects for signal strength and mixed effects for lineup difficulty $d$ and user ability $u$: }
\begin{eqnarray*}
g(\pi_i) &=& \eta_i = \mu + \tau_{j(i)} +\delta_{k(i)}+ \nu_{s(i)} + u_{u(i)} + d_{d(i)},\\
Y &=& g^{-1}(\eta) + \varepsilon
\end{eqnarray*}
\alnote{We haven't introduced $\sigma$, but we include it below, so we need to add to this model. Simply adding: $y_i = g^{-1}(\eta) + \varepsilon$ (after defining $eta$) and including $\varepsilon\sim \mathcal{N}(0, \sigma^2)$ would fix this. Does this make it too unreadable?}
\hhnote{how about that? - I'm fairly certain that lmer is not making a normal assumption when it fits a Binomial, but of course I don't know the inner workings ...}
where $g$ is the logit link function, and $j(.), k(.), s(.), u(.)$, and $d(.)$ are  indexing functions that relate evaluation $i$ to the corresponding levels in the factor variables, to the observer, or a particular data sample. More specifically, $j(i) \in \{$Control, Standard, De-trended$\}$; $k(i) \in \{2,5,10\}$; $s(i) \in \{20, 30, 50, 75\}$; $u(i)$ maps to the participant's id of the $i$th evaluation and $d(i)$ identifies the particular data sample used for it. 
Both user ability, $u$, and sample difficulty, $d$, are modeled as independent, normally distributed  random effects, i.e. $u_{u(i)} \sim N(0, \sigma_u^2)$, $d_{d(i)} \sim N(0,\sigma_d^2)$ with cov$(u, d) = 0$.

\hh{We further assume that $E[\varepsilon] = 0$ and Var$[\varepsilon]=\sigma^2$.}


<<model, echo=FALSE, dependson='data', message=FALSE>>=
library(lme4)
# takes quite a bit of time to fit
lm0 <- glm(correct~treatment+placement+choice+ df + sample_size, family=quasibinomial(), data=turkw, weight=weight)

# after making sure that this actually converges, we can suppress Warnings (this is about the non-integer response, which can't be helped)
turkw$df <- factor(turkw$df, levels=c("10", "5", "2"))
m0 <- suppressWarnings(glmer(correct~treatment+ df + sample_size +(1|ip_address) + (1|difficulty), family=binomial(), data=turkw, weight=weight, control=glmerControl(optimizer="bobyqa")))
@

% \begin{figure}
% \begin{subfigure}[b]{0.5\textwidth}
% <<abilities, dependson='model', echo=FALSE, fig.height=2.5>>=
% qplot(`(Intercept)`, geom="histogram", data=ranef(m0)$ip_address, binwidth=0.05, fill=I("grey35")) + xlab("Individuals' ability u") + theme_bw()
% @
% \end{subfigure}
% \begin{subfigure}[b]{0.5\textwidth}
% <<abilities2, dependson='model', echo=FALSE, fig.height=2.5>>=
% qplot(`(Intercept)`, geom="histogram", data=ranef(m0)$difficulty, fill=I("grey35"), binwidth=0.5) + xlab("data difficulty level d") + theme_bw()
% @
% \end{subfigure}
% \caption{\label{fig:ranef}Histograms of random effects of individuals' abilities (left) and difficulty level of the data (right). }
% \end{figure}

% latex table generated in R 3.1.0 by xtable 1.7-3 package
% Sun Aug  3 11:20:58 2014
% xtable(summary(m0)$coefficients, digits=c(0,2,3,2,4))
\begin{table}[ht]
\centering
\caption{\label{tab:model} Coefficients and significances corresponding to  model $M_1$. The type of design is important for the power of a lineup. De-trended Q-Q plots lose a significant amount of power compared to both the regular and the standard version of Q-Q plots. }
\begin{tabular}{rrrrrl}
  \hline
 &\bf Estimate &\bf Std. Error &\bf z value &\bf Pr($>$$|$z$|$) & \\ 
  \hline
  Intercept &  -5.37 & 0.769 & -6.98 & 0.0000  & *** \\ [3pt]
\multicolumn{3}{l}{\bf design} \\
   Control & 0.00 & ----- & ----- & ----- \\ 
   Standard & 0.06 & 0.103 & 0.62 & 0.5371 \\
   De-trended & -0.50 & 0.104 & -4.77 & 0.0000 & ***\\  [3pt]
\multicolumn{4}{l}{\bf degrees of freedom} \\
  2 & 6.63 & 0.752 & 8.82 & 0.0000 & ***\\ 
  5 & 2.65 & 0.732 & 3.61 & 0.0003 & **\\ 
  10 & 0.00 & ----- & ----- & ----- \\ [3pt]
\multicolumn{3}{l}{\bf sample size} \\
  20 & 0.00 & ----- & ----- & ----- \\ 
  30 & 0.88 & 0.848 & 1.03 & 0.3014 \\ 
  50  & 3.26 & 0.837 & 3.90 & 0.0001 & ***\\ 
  75 & 2.20 & 0.838 & 2.63 & 0.0086  & **\\ 
   \hline
\multicolumn{6}{l}{Signif. codes:  0 $\le$ *** $\le$ 0.001 $\le$ ** $\le$ 0.01 $\le$ * $\le$ 0.05 $\le$ . $\le$ 0.1 $\le$ ' ' $\le$ 1}
\end{tabular}
\end{table}

The estimated model coefficients for model $M_1$ are shown in Table~\ref{tab:model}. 
Estimates of the variance components are $\widehat{\sigma}_u = \Sexpr{round(m0@theta[1],2)}$, $\widehat{\sigma}_d=\Sexpr{round(m0@theta[2],2)}$, and $\widehat{\sigma} = \Sexpr{round(sqrt(sum((fitted(m0)-turkw$correct)^2)/(nrow(turkw)-8)),2)}$. Variances of user ability and data difficulty are large relative to residual variance, indicating that both random effects are necessary.
%
%Figure~\ref{fig:ranef} shows histograms of the predicted random effects for participants' abilities (left) and difficulty level of lineups (right). 
Compared to difficulty level of lineups, participants' abilities only vary little. The difference between best and worst performance by participants has an effect of at most an estimated 
\Sexpr{require(lme4); eff <- range(ranef(m0)[["ip_address"]][,1]); probs <- exp(eff)/(1+exp(eff)); round(probs[2]/probs[1],1)}-fold probability of detecting the data plot from a lineup. 

%------------------------------------------------------------------------------------
\section{Power: three different designs of Q-Q plots}\label{sec:power1}
%-----------------------------------------------------------------------------------

%
As expected, the task of identifying non-normality becomes easier with increased sample size and more pronounced deviations from normality due to lower degrees of freedom. The  design of the Q-Q plot is of huge importance for the probability of choosing the data plot: compared to the control chart, add-on confidence bands helps with evaluation in the standard design, but the difference is not significant.  Surprisingly, the de-trended version of the Q-Q plot is significantly less powerful in detecting non-normality than either of the other designs. 
\hh{In terms of rejections of the null hypothesis, this means that normality is rejected in 24 out of the 48 lineups of the de-trended Q-Q plot. All of these cases are being rejected in all of the other designs as well. Under the control design another four lineups are rejecting normality, and the standard design rejects yet another lineup.}

% \begin{table}[ht]
% \centering
% \caption{\label{tab:reject1}
% %
%  From left to right, we see the number of rejections from different lineup designs.
%   Out of the 24 non-normal samples, 12 get rejected at the 5\% significance level based on evaluation of the de-trended design. Besides these 12 normality is rejected for another two samples by lineups in the control design. The standard design detects non-normality in yet another design. }
% 
% \begin{tabular}{rrrr}
%   \hline
%  & Standard & Control & De-trended  \\ 
%   \hline
%   \hline
%   reject $N(0,1)$ & 15 & 14 & 12    \\ 
% \end{tabular}
% \end{table}
\hh{To investigate the difference between the standard and the de-trended design  further, consider figure \ref{fig:rotstd}. Here, we have an example of a sample, which is rejected based on a  lineup in the standard design, but not from a lineup of de-trended Q-Q plots. Instead of the panel showing the sample, observers focus on panel \#$(3^2-2)$ (with 18 out of 21 picks). This panel was picked out as most different 9 out of 27 times in the standard design, too, indicating that there is something special about it, but most observers (16 out of 27) picked the data in panel \#$(2^2+1)$ from the standard design. }
\begin{figure}[hbt]
\begin{subfigure}{0.5\textwidth}
<<notrotated, dependson='data', echo=FALSE, fig.width=6, fig.height=6>>=
frames <- subset(lppvals, Standard < 0.05 & Rotated > 0.5)$data_name
dframe <- read.csv(sprintf("lineup-data/%s",frames[1]))
dframe$.sample <- dframe$.sample_outer                   
std_lineup(dframe)
@
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
<<notrotated2, dependson='data', echo=FALSE, fig.width=6, fig.height=6>>=
rot_lineup(dframe)
@
\end{subfigure}
\caption{\label{fig:rotstd}Lineups of the same data in two different designs. In the standard design the data sample (in panel \#$(2^2+1)$) is identified 16 out of 27 times, leading to a rejection of normality. The same data set is only identified once out of 21 times in the lineup showing the de-trended design. Observers instead pick panel \#$(3^2-2)$ 18 times. }
\end{figure}
%xtabs(weight~response, data=subset(turkw, data_name==frames[1] & plot_location==5 & treatment=="Standard"))
%xtabs(weight~response, data=subset(turkw, data_name==frames[1] & plot_location==5 & treatment=="Rotated"))
\hh{Two of the observers picking the `wrong' plot from the de-trended lineup gave as a reason for their choice that this panel was the one with `most dots outside the shaded area', i.e.~they focussed on the middle of plot \#7. This is not a singular occurrence -- when investigating  overall reasoning we take a closer look at the effect of the  words `area' and `outside' in the reason participants gave for making their choice of plot (figure~\ref{fig:rotfalse}). As expected, these words barely occur in the control design (where there is no shaded area). It seems to help for identifying the data plot in the standard design, but it severely  increases the chance of picking a null plot in the de-trended design. Why is that? -- The de-trended version is making better use of the space in the plot, it therefore  emphasizes deviations of points from the $x$-axis, i.e.~the theoretical distribution, and with it  the fact whether individual points are inside or outside the shaded areas corresponding to the (pointwise) 95\% confidence intervals. The responses suggest that participants take the shading very seriously and make their choice dependent on it. It also seems that the confidence bands mislead people -- this suggests, that for the de-trended Q-Q plot design we might have to re-think how to display confidence intervals: it might be better to  either use  a more conservative  confidence level or change the approach altogether from pointwise confidence intervals to simultaneous confidence bands as, for example, discussed by \citet{Rosenkrantz:2000fd}.}
% \alnote{This would be a very natural place to put in TAS citations: Aldor-Noiman et al (2013) and Rosenkrantz (2000). }
% \hhnote{comment on Rosenkrantz is missing still}

\hh{Another promising approach might be to base confidence bands on the TS test \citep{buja:2013}. These bands define a test of normality and are narrower in the tails than those associated with the Kolmogorov-Smirnov test, while slightly more conservative in the middle of the distribution. These findings coincide with our observation, that points  outside the confidence intervals were misleading participants, if this occurred in the middle of the plot.
}

<<choice, echo=FALSE>>=
parse_choice <- function(x) {
  x <- as.character(x)

  choices <- as.data.frame(matrix(rep(FALSE,length(x)*5), nrow=length(x), ncol=5))
  names(choices) <- c("outlier", "left", "right","curve", "other")
  choices$outlier[grep("1", x)] <- TRUE
  choices$left[grep("2", x)] <- TRUE
  choices$right[grep("3", x)] <- TRUE
  choices$curve[grep("4", x)] <- TRUE
  choices$other <- gsub(".*_(.*)","\\1",x)
  choices$x
  choices
}

ch <- parse_choice(turkw$choice_reason)
turkw <- data.frame(turkw, ch)
trt <- ddply(turkw, .(treatment, correct), summarise,
      n=sum(weight),
      outlier=sum(outlier*weight),
      left=sum(left*weight),
      right=sum(right*weight),
      curve=sum(curve*weight),
      other=paste(other, collapse=", ")
      )
levels(trt$treatment)[2] <- "De-trended"
require(reshape2)
trt$choice <- "null"
trt$choice[trt$correct] <- "data"
trtm <- melt(trt,id.var=c("treatment", "n","choice"), measure.var=c("outlier", "left", "right", "curve"))
trtm$pct <- trtm$value/trtm$n*100


trt$other <- gsub(",","", trt$other)
trt$other <- gsub(" +"," ", trt$other)
trt$other <- tolower(trt$other)

#trt$above <- unlist(llply(gregexpr("above",trt$other), length))
trt$area <- unlist(llply(gregexpr("area",trt$other), length))
trt$outside <- unlist(llply(gregexpr("outside",trt$other), length))
#trt$test <- unlist(llply(gregexpr("below",trt$other), length))
@
\begin{figure}[hbt]
\centering
<<rotfalse,echo=FALSE, fig.width=6, fig.height=3, out.width='.65\\textwidth'>>=
trtm2 <- melt(trt,id.var=c("treatment", "n","choice"), measure.var=c("area", "outside"))
qplot(treatment, fill=choice, weight=value, data=trtm2, position="dodge") + scale_fill_brewer(palette="Set2") + facet_wrap(~variable) + theme_bw()
@
\caption{\label{fig:rotfalse}Mentioning `outside' or `area' in the reason for selecting the plot from the lineup increases the probability of not identifying the data plot by a large factor in de-trended Q-Q plots. }
\end{figure}

\begin{figure}[hbt]
\centering
<<trtplot, echo=FALSE, fig.width=10, fig.height=4, out.width='\\textwidth'>>=
qplot(treatment,fill=choice, weight=pct, geom="bar", position="dodge",data=trtm) +
  facet_grid(.~variable) + theme_bw() + ylab("percent") + 
  theme(legend.position="bottom") + 
  scale_fill_brewer(palette="Set2")

@
\caption{\label{fig:choices}Overview of reasons participants gave for  their answer. `Outliers' as a reason drastically improves the chance of identifying the data plot. All the other reasons either have no effect or decrease the chance of picking the data. It is curious to see that so many more observers respond `left side different' over `right';  all the samples  come from a  $t$-distribution, so deviations in the extremes should therefore also be symmetric.}
\end{figure}

\hh{Figure~\ref{fig:choices} summarizes the reasons participants gave for the choice of plot they selected. Bars on the left show  identifications of the data plot, bars on the right represent selections of a null plot. The reasons represent the four reasons offered  to participants for check marking. Notably the reasons do not seem to differentiate between the three designs. Stating `outliers' as a reason for choosing a plot is helpful across all designs in picking the data out of the lineup. Stating `left side different' or `points curve'  as the reason for choosing a plot decreases the chance for this plot to be the data plot. `Right side different' does not seem to have any effect. Interestingly, there is a big difference in the percentages of `right' and `left'. Participants favored to give `left side different' as a reason over `right' side, even though all distributions involved were symmetric and therefore deviations from normality should also manifest themselves in a symmetric fashion.}

%\hhnote{another idea: go through reasons for choices. For standard design the line was often interpreted as a line of fit (how often for the de-trended design?), ie. we are tapping into what general population knows about regression. While this is not completely appropriate, the knowledge transfer might be the reason for the better performance.}

%------------------------------------------------------------------------------------
\section{Power: visual and classical}\label{sec:power2}
%-----------------------------------------------------------------------------------

Note that none of the data plots in the lineups were actually created using data from a normal distribution. This should lead to rejection of the null hypothesis in every single instance. This is not quite true, as can be seen in Table~\ref{tab:reject}, but what also becomes evident is the high power  of visual inference. Based on lineups we are able to reject non-normality much more often than with any of the classical tests.


% latex table generated in R 3.0.1 by xtable 1.7-1 package
% Mon May 27 20:57:50 2013
\begin{table}[ht]
\centering
\caption{\label{tab:reject}
 From left to right, we see the number of rejections from visual inference as well as the  Shapiro-Wilk, Anderson-Darling, Lilliefors,  Kolmogorov-Smirnov, and Cram\'er-von Mises tests for normality. Out of the 24 non-normal samples, 12 get rejected at the 5\% significance level based on evaluation by observers. None of the standard normal tests come close to that rejection rate. The power we observe here, matches with the power discussion by \citet{razali:2011} for the SW, AD, and the LF  test. The number in parentheses are the number of situations in which the Standard Q-Q plot agrees with the normal test in rejecting the sample.}

\begin{tabular}{rrrrrrrrr}
  \hline
 & Standard  & SW & AD & LF   & CVM & \\ 
  \hline
  \hline
%  reject $N(0,1)$ & 15 & 14 & 12  &   &  &   &  & 2\\ 
%  not reject & 19 & 20 & 22 &  &   &  &   & 44\\ 
%   \hline
  reject $N(0,S^2)$  & 12  &  8 (7) & 5 (5) &  5 (5) & 4 (4) & \\ 
%  not reject &  &  &  & 32 & 38  & 38 &  40 & \\ 
\hline
\end{tabular}
\end{table}
\hh{
Figure~\ref{fig:visnorm} shows a scatterplot of $p$-values from Shapiro Wilks and estimated $p$ values from the lineup of Q-Q plots in the Standard design. Out of the 24 samples, the tests agree on 18, of which seven are rejections. Of the remaining six, five get rejected only by the visual test, and one is rejected by Shapiro Wilks, but not by the visual test. Two samples on which the tests disagree are circled in figure~\ref{fig:visnorm}, and followed up in more detail. Two lineups corresponding to these observations are shown in figure~\ref{fig:lpnorm}. The lineup on the left corresponds to a sample that is rejected by Shapiro Wilks, but is not rejected by the visual test: only 1.5 decisions (at least one observer picked two panels in his/her response) out of 38 identified the data panel as the most different, which is not enough to reject the null hypothesis of $N(0, S^2)$. 
In contrast to that, the data plot in the  lineup on the right is picked by 23 out of 26 independent observers, leading to a very clear rejection. 
The corresponding $p$-value by Shapiro Wilks is 0.2318, after Lilliefors (0.1689) the test with the lowest $p$-value on this data sample out of all the normality tests.
}
\hh{The difference in significance between normality tests and the visual test might be the way the theoretical distribution is chosen against which the sample is compared. The normality tests are based on sample mean and sample variance. Both of these estimates are affected by outliers. Compared to a normal distribution, the samples from a $t$ distribution exhibits heavier tails. In a finite sample, the heavier tails might look like outliers. By taking these outliers into account, the normality tests lose a lot of power. The Q-Q plots are based on a robust estimation of scale based on the middle half of the empirical distribution. Q-Q plot are therefore less affected by outliers and tails of a $t$ distribution are more easily distinguishable from tails of a normal distribution as can be seen in the lineup on the right of figure~\ref{fig:lpnorm}. Compare this to the lineup of figure~\ref{fig:lp3}, which is based on the same data, but the nulls are sampled from a normal distribution with a variance estimated as the sample variance. The data plot does not stand out, and we would not reject the null hypothesis based on this lineup. }
\hh{An inferior performance of normality tests based on sample mean and variance is also observed by \citep{buja:2013} in the discussion of the TS test. }
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/figvisnorm.pdf} 
\caption{\label{fig:visnorm}  Scatterplot of $p$-values from Shapiro-Wilks and estimated $p$-values from lineups of Standard Q-Q plots. The grey shaded areas represent areas of rejection under at least one of the tests. The circled observations correspond to samples that lead to decidedly different decisions under the two tests. The lineups corresponding to these observations are shown in figure~\ref{fig:lpnorm}.}
\end{figure}

\begin{figure}
\begin{subfigure}[t]{.5\textwidth}
<<plot, echo=FALSE, results='asis', fig.width=6, fig.height=6, message=FALSE>>=
dframe <- read.csv("lineup-data/data-2-1-6-30-5-14-15.csv")
dframe$.sample <- dframe$.sample_outer

fit2 <- dframe
b <- (HLMdiag:::qqlineInfo(dframe$x[dframe$.n==20]))[2] # get robust estimate for sigma
fit2$x[fit2$.n==20] <- dframe$x[dframe$.n==20]/b # change variance to 1, compare then
idx <- grep("naive1.qq", names(fit2))
fit2 <- ddply(fit2[,-idx], .(.n), transform, naive1.qq=qqnorm(x, plot.it=FALSE))
idx <- grep("naive1.env", names(fit2))
fit2 <- ddply(fit2[,-idx], .(.n), transform, naive1.env=sim_env(x))
std_lineup(fit2)

require(xtable)
dx <- matrix(0, ncol=4, nrow=5)
dt <- as.data.frame(xtabs(weight~response, data=t15w, data_name=="data-2-1-6-30-5-14-15.csv" & obs_plot_location==15))
dt$response <- as.numeric(as.character(dt$response))
dx[dt$response] <- dt$Freq
#
dx <- prettyNum(dx, digits=2, zero.print="")
dx[15] <- sprintf("\\bf \\it %s", dx[15])
dx <- matrix(dx, ncol=4, nrow=5)
dx1 <- dx
@
\hfill
%
\begin{tabular}{C{0.75cm}|C{0.75cm}|C{0.75cm}|C{0.75cm}|c}
 <<t1, dependson='plot1', results='asis', echo=FALSE>>=
print(xtable(t(dx1)), floating=FALSE, only.contents=TRUE, include.rownames=FALSE, include.colnames=FALSE, zero.print="\\phantom{0.00}",  sanitize.text.function = function(x){x}, hline.after=1:3)
@
\end{tabular}
\hfill

\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}
<<plot2, echo=FALSE, results='asis', fig.width=6, fig.height=6>>=
dframe <- read.csv("lineup-data/data-2-2-17-20-5-8-15.csv")
dframe$.sample <- dframe$.sample_inner

fit2 <- dframe
b <- (HLMdiag:::qqlineInfo(dframe$x[dframe$.n==20]))[2] # get robust estimate for sigma
#sd(fit2$x[fit2$.n==20])
fit2$x[fit2$.n==20] <- dframe$x[dframe$.n==20]/b # change variance to 1, compare then
idx <- grep("naive1.qq", names(fit2))
fit2 <- ddply(fit2[,-idx], .(.n), transform, naive1.qq=qqnorm(x, plot.it=FALSE))
idx <- grep("naive1.env", names(fit2))
fit2 <- ddply(fit2[,-idx], .(.n), transform, naive1.env=sim_env(x))
std_lineup(fit2)
fit3 <- dframe
b <- sd(dframe$x[dframe$.n==20]) # get robust estimate for sigma
#sd(fit2$x[fit2$.n==20])
fit3$x[fit3$.n==20] <- dframe$x[dframe$.n==20]/b # change variance to 1, compare then
idx <- grep("naive1.qq", names(fit3))
fit3 <- ddply(fit3[,-idx], .(.n), transform, naive1.qq=qqnorm(x, plot.it=FALSE))
idx <- grep("naive1.env", names(fit3))
fit3 <- ddply(fit3[,-idx], .(.n), transform, naive1.env=sim_env(x))
lp3 <- std_lineup(fit3)
dx <- matrix(0, ncol=4, nrow=5)
dt <- as.data.frame(xtabs(weight~response, data=t15w, data_name=="data-2-2-17-20-5-8-15.csv" & obs_plot_location==8))
dt$response <- as.numeric(as.character(dt$response))
dx[dt$response] <- dt$Freq

dx <- prettyNum(dx, digits=2, zero.print="")
dx[8] <- sprintf("\\bf \\it %s", dx[8])
dx <- matrix(dx, ncol=4, nrow=5)
dx2 <- dx
@
\hfill
%
\begin{tabular}{C{0.75cm}|C{0.75cm}|C{0.75cm}|C{0.75cm}|c}
 <<t2, dependson='plot2', results='asis', echo=FALSE>>=
print(xtable(t(dx2)), floating=FALSE, only.contents=TRUE, include.rownames=FALSE, include.colnames=FALSE, zero.print="\\phantom{0.00}",  sanitize.text.function = function(x){x}, hline.after=1:3)
@
\end{tabular}
\hfill

\end{subfigure}
\caption{\label{fig:lpnorm}  On the left, results are not significant, on the right they are highly significant. These two lineups correspond to the results circled in~fig~\ref{fig:visnorm}. The tables below the lineups show the number of times each of the panels was picked as the most different. Non-integer numbers result from multiple choice plots. The italicized numbers refer to the panel that contains the actual sample.
}
\end{figure}

\begin{figure}
\centering
<<lp3, echo=FALSE, fig.width=6, fig.height=6, out.width='0.5\\textwidth'>>=
lp3
@
\caption{\label{fig:lp3} Lineup in standard design showing the data of the lineup on the right in figure~\ref{fig:lpnorm}. The hypothesized distribution is $N(0, \widehat{\sigma}^2)$, where $\widehat{\sigma}^2$ is estimated as the regular (i.e.~non-robust) sample variance, and nulls are drawn from that distribution. While not actually user tested, we do not think that the data stands out from the lineup.}
\end{figure}
%-----------------------------------------------------------------------------------
%------------------------------------------------------------------------------------
\section{Discussion and Conclusion}
%-----------------------------------------------------------------------------------
\hh{In the comparison of the three designs of Q-Q plots, de-trended Q-Q plots turn out to have significantly less power in detecting non-normality than Q-Q plots in the standard and the control design. This comes as a surprise, as results from cognitive psychology suggest that the de-trended version has superior qualities. From the additional reasoning provided by participants regarding their choice of plot, it becomes obvious that this choice is mainly driven by points outside the shaded area depicting 95\% confidence intervals. This happens primarily in the middle of the distribution, confirming results by \citet{buja:2013} and reopens the question of whether the design or the choice of confidence calculation is the reason for the inferiority. It would also make sense to fix the aspect ratio of plots in the de-trended design to make comparisons between the range of points in both axis directions possible.}
\hh{All versions of Q-Q plots under consideration here are significantly better at detecting deviations from normality than classical normality test. A contributing factor to this superior power might be  that in  Q-Q plots the whole sample is assessed rather than being reduced to the single value considered for the test statistic of the normality tests. 
Contributing to the power is also the robust approach in estimating the parameters for the normal distribution drawn as a line of fit in  Q-Q plots, while most normality tests are based on the outlier sensitive sample variance. %$S^2 = \sum_i(x_i - \bar{X})^2/(n-1)$ 
This is consistent with findings in \citet{buja:2013} and  also poses the question of whether power of classical normality tests might  be improved by using robust estimates of the sample.

Q-Q plots are not restricted to assessing normality. In fact, they provide a general framework for testing distributional assumptions. Used in the setting of lineups, they in particular allow an assessment of 
 samples from `approximate' normal or asymptotic normal distributions, as long as there exists a method  to generate data under the null hypothesis, which can then be used to provide null plots in the lineup.
}
\hh{One of the problems with lineups is that they come at a cost, both monetary and in time, that is higher than that of classical testing.
But developments such as {\tt nullabor} \citep{nullabor} or {\tt vistest} \citep{vistest} allow us to be, at least to a degree, our own testers. For tests that are of a more sensitive nature, the cost of a test using a crowd-sourcing service is certainly a small enough item in the overall project budget that it is a feasible 
option. It also discourages the analyst from multiple testing!}

\hh{Several possibilities for immediate extensions are obvious: the simulation study here is only concerned with deviations from normality as given by the $t$-distribution. Other types of deviations, such as given by skew distributions or mixed distributions, would be interesting to consider as well. We doubt that the overall results would change dramatically, but it might give us more insight into what observers of plots consider in making their assessments. }
\hh{The application of the lineup framework based on the related P-P plots poses a natural next question: \citet{koehler91} comment on the higher sensitivity of  P-P  to discrepancies in the middle of the distribution, such as caused by multiple modes. We can verify this and other statements for large samples and based on distributions. Lineups allow us to quantify the extent to which these statements hold for small sample sizes.}


\bibliographystyle{asa}
\bibliography{qqplots}

\newpage
\begin{appendix}

\section{Demographics and other study results}
%-----------------------------------------------------------------------------------

<<attempts, echo=FALSE, dependson='data'>>=
suppressMessages(require(Hmisc))
attempts <- ddply(turkw, .(attempt), summarise, 
                  cmean=wtd.mean(correct, weights=weight, na.rm=TRUE),
                  csd=sqrt(wtd.var(correct, weights=weight, na.rm=TRUE)/sum(weight)),
                  tt=wtd.mean(time_taken, weights=weight, na.rm=TRUE),        
                  sdtt=sqrt(wtd.var(time_taken, weights=weight, na.rm=TRUE)/sum(weight)),
                  nevals=sum(weight)
                  )
@

 Similar to observations made in other lineup experiments \citep{Majumder:2014up}, there is no indication of a `learning' effect within a participant's first ten answers (see figure~\ref{fig:attempts}), indicating that lineups make use of an observer's inherent ability rather than a learned skill. The overall proportion of correct responses is very stable, fluctuating around \Sexpr{round(with(turkw, Hmisc::wtd.mean(correct, weights=weight)),2)} with a standard deviation of about \Sexpr{round(mean(attempts$csd),2)}.  What does change with the number of attempts, is the time taken by participants to answer a lineup. From initially \Sexpr{round(attempts$tt[1],1)} seconds in the first answer, the response time drops to \Sexpr{round(attempts$tt[10],1)} seconds at the tenth evaluation.
\begin{figure}
\centering
\begin{subfigure}[b]{.35\textwidth}
<<fig1, echo=FALSE, dependson='attempts', fig.width=4, fig.height=4>>=
qplot(attempt, cmean, data=subset(attempts, attempt <= 10)) + 
  ylim(c(0,1)) + 
  scale_x_discrete(breaks=1:10) + 
  geom_segment(aes(x=attempt, xend=attempt, y=cmean+1.96*csd, yend=cmean-1.96*csd, group=attempt)) +
  ylab("Proportion of correct responses") + theme_bw()
@
\end{subfigure}
\begin{subfigure}[b]{.35\textwidth}
<<fig2, echo=FALSE, dependson='attempts', fig.width=4, fig.height=4>>=
qplot(attempt, tt, data=subset(attempts, attempt <= 10)) + 
  ylim(c(0,60)) + 
  scale_x_discrete(breaks=1:10) + 
  geom_segment(aes(x=attempt, xend=attempt, y=tt+1.96*sdtt, yend=tt-1.96*sdtt, group=attempt)) +
  ylab("Response time in seconds") + theme_bw()
@
\end{subfigure}
\caption{\label{fig:attempts}Proportion of correct answers by attempt (left) and time taken in each of the first ten attempts (right). The proportion of correct responses stays constantfor successive attempts, while the time to answer decreases significantly over the same number of attempts.}
\end{figure}

\hhnote{gender, education, geographic location?}
\end{appendix}

\end{document}